# kubernetes/apps/airflow/airflow/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: airflow
spec:
  interval: 15m
  chart:
    spec:
      chart: airflow
      version: "1.16.0" # Replace with desired version
      sourceRef:
        kind: HelmRepository
        name: apache-airflow
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    # REQUIRED for Flux - Disable Helm hooks
    createUserJob:
      useHelmHooks: false
      applyCustomEnv: false
    migrateDatabaseJob:
      useHelmHooks: false
      applyCustomEnv: false

    # Enable standard naming (recommended for new installations)
    useStandardNaming: true

    # Basic configuration
    executor: "KubernetesExecutor" # Or CeleryExecutor, etc.

    # Database configuration
    data:
      # Use the existing airflow-metadata secret that contains the connection string
      metadataSecretName: airflow-metadata
      # metadataConnection is not needed when using metadataSecretName

    # Ingress configuration
    ingress:
      web:
        enabled: true
        ingressClassName: "internal" # or "external" based on your setup
        hosts:
          - name: "airflow.${SECRET_DOMAIN}"
            tls:
              enabled: true
              secretName: "${SECRET_DOMAIN/./-}-production-tls"
        annotations:
          cert-manager.io/cluster-issuer: "letsencrypt-production"

    # Database configuration (using built-in PostgreSQL)
    postgresql:
      enabled: true
      auth:
        enablePostgresUser: true
        existingSecret: "airflow-postgresql-secret"
        secretKeys:
          adminPasswordKey: "postgres-password"
          userPasswordKey: "password"
        username: "airflow"
        database: "airflow"
      primary:
        persistence:
          storageClass: "ceph-block"

    # Redis configuration (if using CeleryExecutor)
    redis:
      enabled: false # Set to true if using CeleryExecutor
    # If enabled, set a static password for Flux
    # password: "your-static-redis-password"

    # Monitoring
    prometheus:
      enabled: true
      serviceMonitor:
        enabled: true

    # Security
    webserverSecretKey: "" # Will be auto-generated

    # Persistent logging configuration - DISABLED due to chart volume mount conflicts
    # logs:
    #   persistence:
    #     enabled: false

    # Configure Airflow logging for centralized collection
    config:
      logging:
        # Disable remote logging - we're using centralized logging via Loki
        remote_logging: "False"
        logging_level: "INFO"
        fab_logging_level: "WARN"
        # Force task logs to stdout for collection by Alloy
        colored_console_log: "True"
        # Ensure logs are formatted for easy parsing
        log_format: "[%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s"
        simple_log_format: "%%(asctime)s %%(levelname)s - %%(message)s"
        
      # Configure task logging to write to stdout
      core:
        # Ensure task logs go to stdout for Vector/Alloy collection
        logging_config_class: "airflow.config_templates.airflow_local_settings.DEFAULT_LOGGING_CONFIG"
        # Disable file-based log fetching since we use Loki
        log_fetch_timeout_sec: "5"
        log_auto_tailing_offset: "30"
        log_animation_speed: "1000"

    # Resource limits
    webserver:
      # Improved health checks to prevent OOM-related restarts
      livenessProbe:
        initialDelaySeconds: 60  # Allow time for gunicorn startup
        periodSeconds: 30
        timeoutSeconds: 10
        failureThreshold: 5  # Generous threshold to avoid false positives during memory pressure
      readinessProbe:
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      resources:
        requests:
          cpu: "200m"
          memory: "1Gi"  # Increased from 512Mi due to OOM kills
        limits:
          cpu: "1000m"
          memory: "4Gi"  # Increased from 2Gi due to gunicorn OOM kills

    scheduler:
      # Health checks for scheduler daemon
      livenessProbe:
        initialDelaySeconds: 120  # Scheduler takes time to initialize
        periodSeconds: 60  # Check every minute
        timeoutSeconds: 20
        failureThreshold: 5
      resources:
        requests:
          cpu: "200m"
          memory: "512Mi"
        limits:
          cpu: "1000m"
          memory: "2Gi"
      # Ensure unbuffered output for log collection
      env:
        - name: PYTHONUNBUFFERED
          value: "1"

    # If using KubernetesExecutor, configure worker resources
    workers:
      resources:
        requests:
          cpu: "200m"
          memory: "512Mi"
        limits:
          cpu: "1000m"
          memory: "2Gi"
      # Ensure worker pods inherit the database connection from parent
      safeToEvict: true

    # Disable example DAGs in production
    env:
      - name: AIRFLOW__CORE__LOAD_EXAMPLES
        value: "False"
      # Force Python to flush stdout/stderr immediately
      - name: PYTHONUNBUFFERED
        value: "1"
      # Limit gunicorn workers to reduce memory pressure
      - name: AIRFLOW__WEBSERVER__WORKERS
        value: "2"  # Reduced from default 4
      - name: AIRFLOW__WEBSERVER__WORKER_TIMEOUT
        value: "120"
      - name: AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT
        value: "120"
      # Ensure task logs are visible
      - name: AIRFLOW__LOGGING__BASE_LOG_FOLDER
        value: "/opt/airflow/logs"
      - name: AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_LOCATION
        value: "/opt/airflow/logs/dag_processor_manager/dag_processor_manager.log"
      # Configure KubernetesExecutor to keep pods for log collection
      - name: AIRFLOW__KUBERNETES_EXECUTOR__DELETE_WORKER_PODS
        value: "True"  # Changed back to True to auto-cleanup after log collection
      - name: AIRFLOW__KUBERNETES_EXECUTOR__DELETE_WORKER_PODS_ON_FAILURE
        value: "False"
      # Add termination grace period to allow log collection
      - name: AIRFLOW__KUBERNETES_EXECUTOR__WORKER_PODS_CREATION_BATCH_SIZE
        value: "10"
      - name: AIRFLOW__KUBERNETES_EXECUTOR__WORKER_CONTAINER_REPOSITORY
        value: "apache/airflow"
      - name: AIRFLOW__KUBERNETES_EXECUTOR__WORKER_CONTAINER_TAG
        value: "2.10.3"
      # Pass environment variables to worker pods
      - name: AIRFLOW__KUBERNETES_EXECUTOR__NAMESPACE
        value: "airflow"
      - name: AIRFLOW__KUBERNETES_EXECUTOR__AIRFLOW_CONFIGMAP
        value: "airflow-config"

    dags:
      gitSync:
        enabled: true
        repo: "https://github.com/wcygan/anton.git" # Replace with your actual repo URL if different
        branch: "main" # Or your default branch
        subPath: "kubernetes/apps/airflow/airflow/dags" # Path to your DAGs within the repo
        resources:
          requests:
            cpu: "50m"
            memory: "64Mi"
          limits:
            cpu: "100m"
            memory: "128Mi"
      # If you prefer to use a Persistent Volume for DAGs instead of gitSync:
      # persistence:
      #   enabled: false # Set to true to use PVC
      #   # existingClaim: your-airflow-dags-pvc # Optional: if you have an existing PVC
      #   size: 10Gi
      #   storageClassName: local-path # Or your preferred storage class
      #   accessMode: ReadWriteOnce

    # Extra secrets/configmaps without Helm hooks
    extraConfigMaps: {}

    # Triggerer configuration
    triggerer:
      enabled: true
      # Persistence removed - logs now go to centralized Loki via stdout
      resources:
        requests:
          cpu: "100m"
          memory: "256Mi"
        limits:
          cpu: "500m"
          memory: "1Gi"
      env:
        - name: PYTHONUNBUFFERED
          value: "1"
    
    # Pod template with Vector sidecar for KubernetesExecutor
    # Note: The Airflow Helm chart handles database connection automatically
    # when using the built-in PostgreSQL, so we don't need to add those env vars
    podTemplate: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: placeholder-name
        labels:
          tier: airflow
          component: worker
          release: {{ .Release.Name }}
      spec:
        shareProcessNamespace: true
        volumes:
          - name: vector-config
            configMap:
              name: vector-sidecar-config
        containers:
          - name: base
            env:
              - name: PYTHONUNBUFFERED
                value: "1"
          - name: vector
            image: timberio/vector:0.39.0-debian
            args:
              - --config
              - /etc/vector/vector.yaml
            volumeMounts:
              - name: vector-config
                mountPath: /etc/vector
            env:
              - name: HOSTNAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: AIRFLOW_CTX_DAG_ID
                value: '{{ "{{ dag_id }}" }}'
              - name: AIRFLOW_CTX_TASK_ID  
                value: '{{ "{{ task_id }}" }}'
              - name: AIRFLOW_CTX_RUN_ID
                value: '{{ "{{ run_id }}" }}'
              - name: AIRFLOW_CTX_EXECUTION_DATE
                value: '{{ "{{ execution_date }}" }}'
            resources:
              requests:
                cpu: 5m
                memory: 20Mi
              limits:
                cpu: 25m
                memory: 50Mi
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh", "-c", "sleep 2"]
        terminationGracePeriodSeconds: 10
