# kubernetes/apps/airflow/airflow/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: airflow
spec:
  interval: 15m
  chart:
    spec:
      chart: airflow
      version: "1.16.0" # Replace with desired version
      sourceRef:
        kind: HelmRepository
        name: apache-airflow
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    # REQUIRED for Flux - Disable Helm hooks
    createUserJob:
      useHelmHooks: false
      applyCustomEnv: false
    migrateDatabaseJob:
      useHelmHooks: false
      applyCustomEnv: false

    # Enable standard naming (recommended for new installations)
    useStandardNaming: true

    # Basic configuration
    executor: "KubernetesExecutor" # Or CeleryExecutor, etc.

    # Database configuration
    data:
      metadataConnection:
        user: airflow
        pass: airflow-db-password-changeme # TODO: Move to 1Password/ESO
        protocol: postgresql
        host: airflow-postgresql.airflow.svc.cluster.local
        port: 5432
        db: airflow
        sslmode: disable

    # Create a custom metadata secret with the correct connection string
    extraSecrets: {}

    # Ingress configuration
    ingress:
      web:
        enabled: true
        ingressClassName: "internal" # or "external" based on your setup
        hosts:
          - name: "airflow.${SECRET_DOMAIN}"
            tls:
              enabled: true
              secretName: "${SECRET_DOMAIN/./-}-production-tls"
        annotations:
          cert-manager.io/cluster-issuer: "letsencrypt-production"

    # Database configuration (using built-in PostgreSQL)
    postgresql:
      enabled: true
      auth:
        enablePostgresUser: true
        postgresPassword: postgres-admin-changeme # TODO: Move to 1Password/ESO
        username: "airflow"
        password: airflow-db-password-changeme # TODO: Move to 1Password/ESO - must match data.metadataConnection.pass
        database: "airflow"
      primary:
        persistence:
          storageClass: "ceph-block"

    # Redis configuration (if using CeleryExecutor)
    redis:
      enabled: false # Set to true if using CeleryExecutor
    # If enabled, set a static password for Flux
    # password: "your-static-redis-password"

    # Monitoring
    prometheus:
      enabled: true
      serviceMonitor:
        enabled: true

    # Security
    webserverSecretKey: "" # Will be auto-generated

    # Persistent logging configuration - DISABLED due to chart volume mount conflicts
    # logs:
    #   persistence:
    #     enabled: false

    # Configure Airflow logging
    config:
      logging:
        # Use local file logging instead of trying to fetch from pods
        remote_logging: "False"
        logging_level: "INFO"
        fab_logging_level: "WARN"
        # Force task logs to stdout for collection by Alloy
        colored_console_log: "True"
        
      # Configure task logging to write to stdout
      core:
        # Ensure task logs go to both file and stdout
        logging_config_class: "airflow.config_templates.airflow_local_settings.DEFAULT_LOGGING_CONFIG"
        
        log_auto_tailing_offset: "30"
        log_animation_speed: "1000"

    # Resource limits
    webserver:
      # Improved health checks to prevent OOM-related restarts
      livenessProbe:
        initialDelaySeconds: 60  # Allow time for gunicorn startup
        periodSeconds: 30
        timeoutSeconds: 10
        failureThreshold: 5  # Generous threshold to avoid false positives during memory pressure
      readinessProbe:
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      resources:
        requests:
          cpu: "200m"
          memory: "1Gi"  # Increased from 512Mi due to OOM kills
        limits:
          cpu: "1000m"
          memory: "4Gi"  # Increased from 2Gi due to gunicorn OOM kills

    scheduler:
      # Health checks for scheduler daemon
      livenessProbe:
        initialDelaySeconds: 120  # Scheduler takes time to initialize
        periodSeconds: 60  # Check every minute
        timeoutSeconds: 20
        failureThreshold: 5
      resources:
        requests:
          cpu: "200m"
          memory: "512Mi"
        limits:
          cpu: "1000m"
          memory: "2Gi"

    # If using KubernetesExecutor, configure worker resources
    workers:
      resources:
        requests:
          cpu: "200m"
          memory: "512Mi"
        limits:
          cpu: "1000m"
          memory: "2Gi"

    # Disable example DAGs in production
    env:
      - name: AIRFLOW__CORE__LOAD_EXAMPLES
        value: "False"
      # Force Python to flush stdout/stderr immediately
      - name: PYTHONUNBUFFERED
        value: "1"
      # Limit gunicorn workers to reduce memory pressure
      - name: AIRFLOW__WEBSERVER__WORKERS
        value: "2"  # Reduced from default 4
      - name: AIRFLOW__WEBSERVER__WORKER_TIMEOUT
        value: "120"
      - name: AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT
        value: "120"
      # Ensure task logs are visible
      - name: AIRFLOW__LOGGING__BASE_LOG_FOLDER
        value: "/opt/airflow/logs"
      - name: AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_LOCATION
        value: "/opt/airflow/logs/dag_processor_manager/dag_processor_manager.log"
      # Configure KubernetesExecutor to keep pods for log collection
      - name: AIRFLOW__KUBERNETES_EXECUTOR__DELETE_WORKER_PODS
        value: "False"
      - name: AIRFLOW__KUBERNETES_EXECUTOR__DELETE_WORKER_PODS_ON_FAILURE
        value: "False"
      # Add termination grace period to allow log collection
      - name: AIRFLOW__KUBERNETES_EXECUTOR__WORKER_PODS_CREATION_BATCH_SIZE
        value: "10"
      - name: AIRFLOW__KUBERNETES_EXECUTOR__WORKER_CONTAINER_REPOSITORY
        value: "apache/airflow"
      - name: AIRFLOW__KUBERNETES_EXECUTOR__WORKER_CONTAINER_TAG
        value: "2.10.3"

    dags:
      gitSync:
        enabled: true
        repo: "https://github.com/wcygan/anton.git" # Replace with your actual repo URL if different
        branch: "main" # Or your default branch
        subPath: "kubernetes/apps/airflow/airflow/dags" # Path to your DAGs within the repo
        resources:
          requests:
            cpu: "50m"
            memory: "64Mi"
          limits:
            cpu: "100m"
            memory: "128Mi"
      # If you prefer to use a Persistent Volume for DAGs instead of gitSync:
      # persistence:
      #   enabled: false # Set to true to use PVC
      #   # existingClaim: your-airflow-dags-pvc # Optional: if you have an existing PVC
      #   size: 10Gi
      #   storageClassName: local-path # Or your preferred storage class
      #   accessMode: ReadWriteOnce

    # Extra secrets/configmaps without Helm hooks
    extraConfigMaps: {}

    # Triggerer configuration with Ceph storage
    triggerer:
      enabled: true
      persistence:
        enabled: true
        storageClassName: "ceph-block"
        size: 100Gi
    
    # Pod template with Vector sidecar for KubernetesExecutor
    podTemplate: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: placeholder-name
        labels:
          tier: airflow
          component: worker
          release: {{ .Release.Name }}
      spec:
        shareProcessNamespace: true
        containers:
          - name: base
            env:
              - name: PYTHONUNBUFFERED
                value: "1"
          - name: vector
            image: timberio/vector:0.39.0-debian
            args:
              - --config-yaml
              - |
                sources:
                  task_stdout:
                    type: file
                    include: 
                      - /proc/1/root/dev/stdout
                    read_from: beginning
                  task_stderr:
                    type: file
                    include:
                      - /proc/1/root/dev/stderr
                    read_from: beginning
                transforms:
                  add_metadata:
                    type: remap
                    inputs: ["task_stdout", "task_stderr"]
                    source: |
                      .namespace = "airflow"
                      .pod = get_env_var!("HOSTNAME")
                      .dag_id = get_env_var("AIRFLOW_CTX_DAG_ID") ?? "unknown"
                      .task_id = get_env_var("AIRFLOW_CTX_TASK_ID") ?? "unknown"
                      .run_id = get_env_var("AIRFLOW_CTX_RUN_ID") ?? "unknown"
                      .execution_date = get_env_var("AIRFLOW_CTX_EXECUTION_DATE") ?? "unknown"
                sinks:
                  loki:
                    type: loki
                    inputs: ["add_metadata"]
                    endpoint: "http://loki-gateway.monitoring.svc.cluster.local:80"
                    encoding:
                      codec: json
                    batch:
                      max_bytes: 512000
                      timeout_secs: 0.5
                    labels:
                      namespace: '{{ "{{ namespace }}" }}'
                      pod: '{{ "{{ pod }}" }}'
                      dag_id: '{{ "{{ dag_id }}" }}'
                      task_id: '{{ "{{ task_id }}" }}'
            env:
              - name: HOSTNAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: AIRFLOW_CTX_DAG_ID
                value: '{{ "{{ dag_id }}" }}'
              - name: AIRFLOW_CTX_TASK_ID  
                value: '{{ "{{ task_id }}" }}'
              - name: AIRFLOW_CTX_RUN_ID
                value: '{{ "{{ run_id }}" }}'
              - name: AIRFLOW_CTX_EXECUTION_DATE
                value: '{{ "{{ execution_date }}" }}'
            resources:
              requests:
                cpu: 5m
                memory: 20Mi
              limits:
                cpu: 25m
                memory: 50Mi
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh", "-c", "sleep 2"]
        terminationGracePeriodSeconds: 10
