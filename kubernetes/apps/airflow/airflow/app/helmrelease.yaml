# kubernetes/apps/airflow/airflow/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: airflow
spec:
  interval: 15m
  chart:
    spec:
      chart: airflow
      version: "1.16.0" # Replace with desired version
      sourceRef:
        kind: HelmRepository
        name: apache-airflow
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    # REQUIRED for Flux - Disable Helm hooks
    createUserJob:
      useHelmHooks: false
      applyCustomEnv: false
    migrateDatabaseJob:
      useHelmHooks: false
      applyCustomEnv: false

    # Enable standard naming (recommended for new installations)
    useStandardNaming: true

    # Basic configuration
    executor: "KubernetesExecutor" # Or CeleryExecutor, etc.

    # Database configuration
    data:
      metadataConnection:
        user: airflow
        pass: airflow-db-password-changeme # TODO: Move to 1Password/ESO
        protocol: postgresql
        host: airflow-postgresql.airflow.svc.cluster.local
        port: 5432
        db: airflow
        sslmode: disable

    # Create a custom metadata secret with the correct connection string
    extraSecrets: {}

    # Ingress configuration
    ingress:
      web:
        enabled: true
        ingressClassName: "internal" # or "external" based on your setup
        hosts:
          - name: "airflow.${SECRET_DOMAIN}"
            tls:
              enabled: true
              secretName: "${SECRET_DOMAIN/./-}-production-tls"
        annotations:
          cert-manager.io/cluster-issuer: "letsencrypt-production"

    # Database configuration (using built-in PostgreSQL)
    postgresql:
      enabled: true
      auth:
        enablePostgresUser: true
        postgresPassword: postgres-admin-changeme # TODO: Move to 1Password/ESO
        username: "airflow"
        password: airflow-db-password-changeme # TODO: Move to 1Password/ESO - must match data.metadataConnection.pass
        database: "airflow"
      primary:
        persistence:
          storageClass: "ceph-block"

    # Redis configuration (if using CeleryExecutor)
    redis:
      enabled: false # Set to true if using CeleryExecutor
    # If enabled, set a static password for Flux
    # password: "your-static-redis-password"

    # Monitoring
    prometheus:
      enabled: true
      serviceMonitor:
        enabled: true

    # Security
    webserverSecretKey: "" # Will be auto-generated

    # Persistent logging configuration - DISABLED due to chart volume mount conflicts
    # logs:
    #   persistence:
    #     enabled: false

    # Configure Airflow logging
    config:
      logging:
        # Use local file logging instead of trying to fetch from pods
        remote_logging: "False"
        logging_level: "INFO"
        fab_logging_level: "WARN"
        # Force task logs to stdout for collection by Alloy
        colored_console_log: "True"
        
      # Configure task logging to write to stdout
      core:
        # Ensure task logs go to both file and stdout
        logging_config_class: "airflow.config_templates.airflow_local_settings.DEFAULT_LOGGING_CONFIG"
        
        log_auto_tailing_offset: "30"
        log_animation_speed: "1000"

    # Resource limits
    webserver:
      # Optimize gunicorn configuration to reduce memory usage
      defaultAirflowRepository: apache/airflow
      defaultAirflowTag: "2.10.3"
      # Limit gunicorn workers to reduce memory pressure
      extraEnv:
        - name: AIRFLOW__WEBSERVER__WORKERS
          value: "2"  # Reduced from default 4
        - name: AIRFLOW__WEBSERVER__WORKER_TIMEOUT
          value: "120"
        - name: AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT
          value: "120"
      # Improved health checks to prevent OOM-related restarts
      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 60  # Allow time for gunicorn startup
        periodSeconds: 30
        timeoutSeconds: 10
        failureThreshold: 5  # Generous threshold to avoid false positives during memory pressure
      readinessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      resources:
        requests:
          cpu: "200m"
          memory: "1Gi"  # Increased from 512Mi due to OOM kills
        limits:
          cpu: "1000m"
          memory: "4Gi"  # Increased from 2Gi due to gunicorn OOM kills

    scheduler:
      # Health checks for scheduler daemon
      livenessProbe:
        exec:
          command:
            - sh
            - -c
            - "airflow jobs check --job-type SchedulerJob --local"
        initialDelaySeconds: 120  # Scheduler takes time to initialize
        periodSeconds: 60  # Check every minute
        timeoutSeconds: 20
        failureThreshold: 5
      resources:
        requests:
          cpu: "200m"
          memory: "512Mi"
        limits:
          cpu: "1000m"
          memory: "2Gi"

    # If using KubernetesExecutor, configure worker resources
    workers:
      resources:
        requests:
          cpu: "200m"
          memory: "512Mi"
        limits:
          cpu: "1000m"
          memory: "2Gi"

    # Disable example DAGs in production
    env:
      - name: AIRFLOW__CORE__LOAD_EXAMPLES
        value: "False"
      # Force Python to flush stdout/stderr immediately
      - name: PYTHONUNBUFFERED
        value: "1"
      # Ensure task logs are visible
      - name: AIRFLOW__LOGGING__BASE_LOG_FOLDER
        value: "/opt/airflow/logs"
      - name: AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_LOCATION
        value: "/opt/airflow/logs/dag_processor_manager/dag_processor_manager.log"
      # Configure KubernetesExecutor to keep pods for log collection
      - name: AIRFLOW__KUBERNETES_EXECUTOR__DELETE_WORKER_PODS
        value: "False"
      - name: AIRFLOW__KUBERNETES_EXECUTOR__DELETE_WORKER_PODS_ON_FAILURE
        value: "False"
      # Make tasks log to both file and stdout
      - name: AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS
        value: "log_config.LOGGING_CONFIG"

    dags:
      gitSync:
        enabled: true
        repo: "https://github.com/wcygan/anton.git" # Replace with your actual repo URL if different
        branch: "main" # Or your default branch
        subPath: "kubernetes/apps/airflow/airflow/dags" # Path to your DAGs within the repo
        resources:
          requests:
            cpu: "50m"
            memory: "64Mi"
          limits:
            cpu: "100m"
            memory: "128Mi"
      # If you prefer to use a Persistent Volume for DAGs instead of gitSync:
      # persistence:
      #   enabled: false # Set to true to use PVC
      #   # existingClaim: your-airflow-dags-pvc # Optional: if you have an existing PVC
      #   size: 10Gi
      #   storageClassName: local-path # Or your preferred storage class
      #   accessMode: ReadWriteOnce

    # Extra secrets/configmaps without Helm hooks
    extraConfigMaps: {}

    # Triggerer configuration with Ceph storage
    triggerer:
      enabled: true
      persistence:
        enabled: true
        storageClassName: "ceph-block"
        size: 100Gi
