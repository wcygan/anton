---
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: iceberg-operations-test
  namespace: data-platform
  labels:
    app.kubernetes.io/name: iceberg-operations-test
    app.kubernetes.io/component: test-job
    app.kubernetes.io/part-of: data-platform
spec:
  type: Python
  mode: cluster
  image: apache/spark-py:3.5.5
  imagePullPolicy: IfNotPresent
  mainApplicationFile: local:///app/iceberg_test.py
  sparkVersion: "3.5.5"
  deps:
    jars:
    - https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.2/iceberg-spark-runtime-3.5_2.12-1.5.2.jar
    - https://repo1.maven.org/maven2/org/projectnessie/nessie-spark-extensions-3.5_2.12/0.77.1/nessie-spark-extensions-3.5_2.12-0.77.1.jar
    - https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar
    - https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.367/aws-java-sdk-bundle-1.12.367.jar
  sparkConf:
    spark.kubernetes.namespace: data-platform
    # Iceberg and Nessie configuration
    spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions
    spark.sql.catalog.nessie: org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.nessie.catalog-impl: org.apache.iceberg.nessie.NessieCatalog
    spark.sql.catalog.nessie.uri: http://nessie:19120/api/v2
    spark.sql.catalog.nessie.ref: main
    spark.sql.catalog.nessie.warehouse: s3a://iceberg-test
    # S3 configuration for Ceph
    spark.hadoop.fs.s3a.endpoint: http://rook-ceph-rgw-storage.storage.svc:80
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3a.fast.upload: "true"
    spark.hadoop.fs.s3a.connection.ssl.enabled: "false"
    # Event logging
    spark.eventLog.enabled: "true"
    spark.eventLog.dir: "s3a://iceberg-test/spark-events"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    coreLimit: "2000m"
    memory: "2g"
    serviceAccount: spark-application-sa
    labels:
      version: 3.5.5
      test-type: iceberg-operations
    envFrom:
    - secretRef:
        name: rook-ceph-object-user-storage-iceberg
    volumeMounts:
    - name: test-script
      mountPath: /app
  executor:
    cores: 1
    instances: 2
    memory: "2g"
    labels:
      version: 3.5.5
      test-type: iceberg-operations
    envFrom:
    - secretRef:
        name: rook-ceph-object-user-storage-iceberg
  volumes:
  - name: test-script
    configMap:
      name: iceberg-test-script
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: iceberg-test-script
  namespace: data-platform
data:
  iceberg_test.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, current_timestamp, lit
    from datetime import datetime
    import time
    
    # Initialize Spark session with Iceberg support
    spark = SparkSession.builder \
        .appName("IcebergOperationsTest") \
        .getOrCreate()
    
    print("=== Iceberg Operations Test Started ===")
    print(f"Spark Version: {spark.version}")
    
    # Show available catalogs
    print("\nAvailable catalogs:")
    spark.sql("SHOW CATALOGS").show()
    
    # Create namespace if not exists
    namespace = "test_db"
    print(f"\nCreating namespace: {namespace}")
    try:
        spark.sql(f"CREATE NAMESPACE IF NOT EXISTS nessie.{namespace}")
        print(f"Namespace {namespace} created/verified")
    except Exception as e:
        print(f"Note: {e}")
    
    # List namespaces
    print("\nNamespaces in Nessie catalog:")
    spark.sql("SHOW NAMESPACES IN nessie").show()
    
    # Create Iceberg table
    table_name = f"nessie.{namespace}.employee_data"
    print(f"\nCreating Iceberg table: {table_name}")
    
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            id BIGINT,
            name STRING,
            department STRING,
            salary DOUBLE,
            hire_date DATE,
            last_updated TIMESTAMP
        ) USING iceberg
        PARTITIONED BY (department)
        TBLPROPERTIES (
            'format-version'='2',
            'write.format.default'='parquet',
            'write.metadata.compression-codec'='gzip'
        )
    """)
    
    print(f"Table {table_name} created successfully")
    
    # Insert sample data
    print("\nInserting sample data...")
    spark.sql(f"""
        INSERT INTO {table_name} VALUES
        (1, 'Alice Johnson', 'Engineering', 95000, DATE'2020-01-15', CURRENT_TIMESTAMP()),
        (2, 'Bob Smith', 'Engineering', 85000, DATE'2020-03-20', CURRENT_TIMESTAMP()),
        (3, 'Charlie Brown', 'Sales', 75000, DATE'2021-05-10', CURRENT_TIMESTAMP()),
        (4, 'Diana Prince', 'Sales', 80000, DATE'2021-07-22', CURRENT_TIMESTAMP()),
        (5, 'Eve Wilson', 'HR', 70000, DATE'2022-02-14', CURRENT_TIMESTAMP())
    """)
    
    # Query data
    print("\nQuerying data from Iceberg table:")
    df = spark.sql(f"SELECT * FROM {table_name} ORDER BY id")
    df.show()
    
    # Show table properties
    print("\nTable properties:")
    spark.sql(f"DESCRIBE EXTENDED {table_name}").show(truncate=False)
    
    # Test schema evolution - add a column
    print("\nTesting schema evolution - adding bonus column...")
    spark.sql(f"ALTER TABLE {table_name} ADD COLUMN bonus DOUBLE")
    
    # Insert more data with new column
    spark.sql(f"""
        INSERT INTO {table_name} VALUES
        (6, 'Frank Miller', 'Engineering', 90000, DATE'2022-09-01', CURRENT_TIMESTAMP(), 5000),
        (7, 'Grace Chen', 'Sales', 82000, DATE'2023-01-10', CURRENT_TIMESTAMP(), 3000)
    """)
    
    print("Data after schema evolution:")
    spark.sql(f"SELECT * FROM {table_name} ORDER BY id").show()
    
    # Show table history
    print("\nTable history (snapshots):")
    spark.sql(f"SELECT * FROM {table_name}.history").show(truncate=False)
    
    # Time travel query - read previous version
    print("\nTime travel - reading initial version of the table:")
    try:
        df_v1 = spark.sql(f"SELECT * FROM {table_name} VERSION AS OF 1")
        print(f"Version 1 had {df_v1.count()} rows")
        df_v1.show()
    except Exception as e:
        print(f"Time travel note: {e}")
    
    # Show table snapshots
    print("\nTable snapshots:")
    spark.sql(f"SELECT * FROM {table_name}.snapshots").show(truncate=False)
    
    # Partition statistics
    print("\nPartition statistics:")
    spark.sql(f"SELECT department, COUNT(*) as employee_count, AVG(salary) as avg_salary FROM {table_name} GROUP BY department").show()
    
    # Test update operation
    print("\nTesting UPDATE operation - giving Engineering dept a raise...")
    spark.sql(f"""
        UPDATE {table_name} 
        SET salary = salary * 1.1, 
            last_updated = CURRENT_TIMESTAMP()
        WHERE department = 'Engineering'
    """)
    
    print("Data after update:")
    spark.sql(f"SELECT id, name, department, salary FROM {table_name} WHERE department = 'Engineering' ORDER BY id").show()
    
    # Clean up (optional - comment out to keep table for inspection)
    # print(f"\nDropping table {table_name}")
    # spark.sql(f"DROP TABLE IF EXISTS {table_name}")
    
    print("\n=== Iceberg Operations Test Completed Successfully ===")
    
    spark.stop()