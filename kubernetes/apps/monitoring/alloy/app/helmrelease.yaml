---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: alloy
  namespace: monitoring
spec:
  interval: 30m
  timeout: 15m
  chart:
    spec:
      chart: alloy
      version: 1.1.1  # Latest stable version
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: flux-system
  install:
    crds: Skip
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: Skip
    remediation:
      strategy: rollback
      retries: 3
  values:
    alloy:
      configMap:
        create: true
        content: |
          logging {
            level = "info"
          }
          
          // Kubernetes discovery
          discovery.kubernetes "pods" {
            role = "pod"
          }
          
          // Relabel pods
          discovery.relabel "pods" {
            targets = discovery.kubernetes.pods.targets
            
            // Keep running and recently completed pods
            rule {
              source_labels = ["__meta_kubernetes_pod_phase"]
              regex         = "Pending|Failed|Unknown"
              action        = "drop"
            }
            
            // Add labels
            rule {
              source_labels = ["__meta_kubernetes_namespace"]
              target_label  = "namespace"
            }
            
            rule {
              source_labels = ["__meta_kubernetes_pod_name"]
              target_label  = "pod"
            }
            
            rule {
              source_labels = ["__meta_kubernetes_pod_container_name"]
              target_label  = "container"
            }
            
            rule {
              source_labels = ["__meta_kubernetes_pod_node_name"]
              target_label  = "node"
            }
            
            // Add all pod labels
            rule {
              action = "labelmap"
              regex  = "__meta_kubernetes_pod_label_(.+)"
            }
            
            // Set path to container logs
            rule {
              source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
              target_label  = "__path__"
              separator     = "/"
              replacement   = "/var/log/pods/*$1/$2/*.log"
            }
          }
          
          // Collect logs - using correct component name for Alloy v1.9.1
          loki.source.kubernetes "pods" {
            targets    = discovery.relabel.pods.output
            forward_to = [loki.process.logs.receiver]
          }
          
          // Note: Airflow logs are collected automatically from all pods in the airflow namespace
          // The loki.process stage below extracts DAG and task metadata from log content
          
          // Process logs
          loki.process "logs" {
            forward_to = [loki.write.default.receiver]
            
            // Parse JSON logs if present
            stage.json {
              expressions = {
                output = "log",
                level  = "level",
              }
            }
            
            // Extract level from message if not in JSON
            stage.regex {
              expression = "(?P<level>TRACE|DEBUG|INFO|WARN|ERROR|FATAL)"
            }
            
            // Add labels
            stage.labels {
              values = {
                level = "",
              }
            }
            
            // Special processing for Airflow logs
            stage.match {
              selector = "{namespace=\"airflow\"}"
              
              // Extract Airflow-specific information from log messages
              stage.regex {
                expression = "\\[(?P<timestamp>[^\\]]+)\\] \\{(?P<source>[^}]+)\\}"
              }
              
              // Extract DAG ID from log content
              stage.regex {
                expression = "dag_id=(?P<dag_id>[\\w-]+)"
              }
              
              // Extract task ID from log content
              stage.regex {
                expression = "task_id=(?P<task_id>[\\w-]+)"
              }
              
              // Extract execution date from log content
              stage.regex {
                expression = "execution_date=(?P<execution_date>[\\d-]+T[\\d:+]+)"
              }
              
              // Add Airflow-specific labels if found
              stage.labels {
                values = {
                  dag_id = "",
                  task_id = "",
                  execution_date = "",
                }
              }
            }
            
            // Drop excessive labels to stay under Loki's limit of 15
            stage.label_drop {
              values = [
                "app_kubernetes_io_created_by", 
                "app_kubernetes_io_managed_by",
                "app_kubernetes_io_part_of",
                "pod_template_hash",
                "ceph_daemon_id",
                "ceph_daemon_type",
                "mon_daemon",
                "mgr_role",
                "rook_io_operator_namespace",
                "rook_object_store",
                "controller_revision_hash",
                "service_istio_io_canonical_revision",
                // Airflow-specific labels to drop
                "airflow_executor_done",
                "airflow_version",
                "airflow_worker",
                "kubernetes_executor",
                "run_id",
                "try_number",
                "execution_date",
                // Rook-ceph labels
                "app_kubernetes_io_component",
                "app_kubernetes_io_instance",
                "app_kubernetes_io_name",
                "ceph_osd_id",
                "device_class",
                "encrypted",
                "failure_domain",
                "osd",
                "osd_store",
                "portable",
                "rook_cluster",
                "topology_location_host",
                "topology_location_root",
              ]
            }
            
            // Drop debug logs from noisy sources
            stage.drop {
              source = "namespace"
              expression = "kube-public"
            }
          }
          
          // Write to Loki
          loki.write "default" {
            endpoint {
              url = "http://loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push"
            }
          }
          
          // Export metrics
          prometheus.exporter.self "alloy" {}
          
          // Scrape own metrics
          prometheus.scrape "alloy" {
            targets = prometheus.exporter.self.alloy.targets
            forward_to = [prometheus.remote_write.monitoring.receiver]
          }
          
          // Send metrics to Prometheus
          prometheus.remote_write "monitoring" {
            endpoint {
              url = "http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090/api/v1/write"
            }
          }
    
    # Controller configuration
    controller:
      type: daemonset
      
      # Health checks for log collection pipeline
      readinessProbe:
        httpGet:
          path: /-/ready
          port: 12345
        initialDelaySeconds: 10
        periodSeconds: 10
        timeoutSeconds: 3
        failureThreshold: 3
      livenessProbe:
        httpGet:
          path: /-/healthy
          port: 12345
        initialDelaySeconds: 30
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 3
      
      # Resources (optimized based on actual usage: ~160m CPU, ~400Mi memory)
      resources:
        requests:
          cpu: 50m    # Reduced from 100m (actual usage is lower)
          memory: 256Mi
        limits:
          cpu: 300m   # Reduced from 500m (actual peak is 160m)
          memory: 600Mi # Increased from 512Mi (actual usage is 400Mi)
      
      # Volume configuration for Talos/containerd
      extraVolumes:
        - name: varlog
          hostPath:
            path: /var/log
            type: Directory
      
      extraVolumeMounts:
        - name: varlog
          mountPath: /var/log
          readOnly: true
      
      # Security context
      securityContext:
        privileged: true
        runAsUser: 0
    
    # Service monitor
    serviceMonitor:
      enabled: true