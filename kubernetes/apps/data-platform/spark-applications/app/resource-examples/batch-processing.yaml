---
# Example: Large-scale batch processing job with optimized resource allocation
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: batch-etl-job
  namespace: data-platform
  labels:
    app.kubernetes.io/name: batch-etl-job
    app.kubernetes.io/component: batch-job
    app.kubernetes.io/part-of: data-platform
spec:
  type: Python
  mode: cluster
  image: apache/spark-py:3.5.5
  imagePullPolicy: IfNotPresent
  mainApplicationFile: s3a://iceberg-test/jobs/batch_etl.py
  sparkVersion: "3.5.5"
  # Dynamic allocation for efficient resource usage
  dynamicAllocation:
    enabled: true
    initialExecutors: 2
    minExecutors: 1
    maxExecutors: 10
  sparkConf:
    spark.kubernetes.namespace: data-platform
    # Memory tuning
    spark.executor.memoryOverhead: "1g"
    spark.kubernetes.memoryOverheadFactor: "0.1"
    spark.memory.fraction: "0.6"
    spark.memory.storageFraction: "0.5"
    # Shuffle optimization
    spark.shuffle.compress: "true"
    spark.shuffle.spill.compress: "true"
    spark.io.compression.codec: "lz4"
    # S3 optimization
    spark.hadoop.fs.s3a.committer.name: "directory"
    spark.hadoop.fs.s3a.committer.staging.conflict-mode: "append"
    spark.hadoop.fs.s3a.fast.upload.buffer: "bytebuffer"
    spark.hadoop.fs.s3a.fast.upload.active.blocks: "8"
    # Adaptive query execution
    spark.sql.adaptive.enabled: "true"
    spark.sql.adaptive.coalescePartitions.enabled: "true"
    spark.sql.adaptive.skewJoin.enabled: "true"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 60
    onSubmissionFailureRetries: 5
    onSubmissionFailureRetryInterval: 20
  driver:
    cores: 2
    coreLimit: "2000m"
    memory: "4g"
    serviceAccount: spark-application-sa
    labels:
      workload-type: batch
    nodeSelector:
      workload: data-platform
    tolerations:
    - key: "data-platform"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
  executor:
    cores: 4
    coreLimit: "4000m"
    memory: "8g"
    labels:
      workload-type: batch
    nodeSelector:
      workload: data-platform
    tolerations:
    - key: "data-platform"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
---
# Example: Interactive query job with fast startup
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: interactive-query
  namespace: data-platform
  labels:
    app.kubernetes.io/name: interactive-query
    app.kubernetes.io/component: interactive-job
    app.kubernetes.io/part-of: data-platform
spec:
  type: SQL
  mode: cluster
  image: apache/spark:3.5.5
  imagePullPolicy: IfNotPresent
  mainApplicationFile: local:///opt/spark/work-dir/query.sql
  sparkVersion: "3.5.5"
  sparkConf:
    spark.kubernetes.namespace: data-platform
    # Fast startup optimization
    spark.jars.ivy: "/tmp/.ivy"
    spark.kubernetes.container.image.pullPolicy: "IfNotPresent"
    spark.executor.instances: "2"
    # Query optimization
    spark.sql.adaptive.enabled: "true"
    spark.sql.adaptive.advisoryPartitionSizeInBytes: "64MB"
    spark.sql.broadcastTimeout: "300"
    spark.sql.autoBroadcastJoinThreshold: "10MB"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    coreLimit: "1000m"
    memory: "2g"
    serviceAccount: spark-application-sa
    labels:
      workload-type: interactive
  executor:
    cores: 2
    instances: 2
    memory: "4g"
    labels:
      workload-type: interactive
---
# Example: Streaming job with checkpointing
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: stream-processing
  namespace: data-platform
  labels:
    app.kubernetes.io/name: stream-processing
    app.kubernetes.io/component: streaming-job
    app.kubernetes.io/part-of: data-platform
spec:
  type: Python
  mode: cluster
  image: apache/spark-py:3.5.5
  imagePullPolicy: IfNotPresent
  mainApplicationFile: s3a://iceberg-test/jobs/stream_processor.py
  sparkVersion: "3.5.5"
  sparkConf:
    spark.kubernetes.namespace: data-platform
    # Streaming configuration
    spark.streaming.stopGracefullyOnShutdown: "true"
    spark.streaming.backpressure.enabled: "true"
    spark.streaming.receiver.writeAheadLog.enable: "true"
    spark.streaming.checkpoint.dir: "s3a://iceberg-test/checkpoints/stream-processing"
    # Micro-batch tuning
    spark.streaming.kafka.maxRatePerPartition: "1000"
    spark.streaming.backpressure.initialRate: "100"
  restartPolicy:
    type: Always
    onFailureRetries: -1  # Infinite retries for streaming
    onFailureRetryInterval: 30
  driver:
    cores: 1
    coreLimit: "1000m"
    memory: "2g"
    serviceAccount: spark-application-sa
    labels:
      workload-type: streaming
  executor:
    cores: 2
    instances: 3
    memory: "4g"
    labels:
      workload-type: streaming