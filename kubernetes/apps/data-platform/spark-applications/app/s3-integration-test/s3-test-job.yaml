---
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: s3-integration-test
  namespace: data-platform
  labels:
    app.kubernetes.io/name: s3-integration-test
    app.kubernetes.io/component: test-job
    app.kubernetes.io/part-of: data-platform
spec:
  type: Python
  mode: cluster
  image: apache/spark-py:3.5.5
  imagePullPolicy: IfNotPresent
  mainApplicationFile: local:///app/s3_test.py
  sparkVersion: "3.5.5"
  sparkConf:
    spark.kubernetes.namespace: data-platform
    # S3 configuration for Ceph
    spark.hadoop.fs.s3a.endpoint: http://rook-ceph-rgw-storage.storage.svc:80
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3a.fast.upload: "true"
    spark.hadoop.fs.s3a.connection.ssl.enabled: "false"
    # Event logging to S3
    spark.eventLog.enabled: "true"
    spark.eventLog.dir: "s3a://iceberg-test/spark-events"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    coreLimit: "1000m"
    memory: "1g"
    serviceAccount: spark-application-sa
    labels:
      version: 3.5.5
      test-type: s3-integration
    # S3 credentials from secret
    envFrom:
    - secretRef:
        name: rook-ceph-object-user-storage-iceberg
    volumeMounts:
    - name: test-script
      mountPath: /app
  executor:
    cores: 1
    instances: 2
    memory: "1g"
    labels:
      version: 3.5.5
      test-type: s3-integration
    envFrom:
    - secretRef:
        name: rook-ceph-object-user-storage-iceberg
  volumes:
  - name: test-script
    configMap:
      name: s3-test-script
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: s3-test-script
  namespace: data-platform
data:
  s3_test.py: |
    from pyspark.sql import SparkSession
    import os
    from datetime import datetime
    
    # Initialize Spark session
    spark = SparkSession.builder \
        .appName("S3IntegrationTest") \
        .getOrCreate()
    
    print("=== S3 Integration Test Started ===")
    
    # Test data
    data = [
        (1, "Alice", 34, datetime.now().isoformat()),
        (2, "Bob", 45, datetime.now().isoformat()),
        (3, "Charlie", 23, datetime.now().isoformat()),
        (4, "Diana", 31, datetime.now().isoformat()),
        (5, "Eve", 28, datetime.now().isoformat())
    ]
    
    columns = ["id", "name", "age", "timestamp"]
    
    # Create DataFrame
    df = spark.createDataFrame(data, columns)
    print(f"Created DataFrame with {df.count()} rows")
    
    # Write to S3
    s3_path = "s3a://iceberg-test/test-data/spark-s3-test"
    print(f"Writing data to: {s3_path}")
    
    df.write \
        .mode("overwrite") \
        .parquet(s3_path)
    
    print("Data written successfully")
    
    # Read back from S3
    print(f"Reading data from: {s3_path}")
    df_read = spark.read.parquet(s3_path)
    
    print(f"Read {df_read.count()} rows from S3")
    df_read.show()
    
    # Test S3 listing
    try:
        import subprocess
        # Using AWS CLI if available
        endpoint = os.environ.get('CEPH_EXTERNAL_RGW_ENDPOINTS', 'http://rook-ceph-rgw-storage.storage.svc:80')
        result = subprocess.run([
            'aws', 's3', 'ls', 's3://iceberg-test/test-data/', 
            '--endpoint-url', endpoint.split(',')[0]
        ], capture_output=True, text=True)
        print("S3 listing:")
        print(result.stdout)
    except Exception as e:
        print(f"AWS CLI not available or error: {e}")
    
    print("=== S3 Integration Test Completed Successfully ===")
    
    spark.stop()