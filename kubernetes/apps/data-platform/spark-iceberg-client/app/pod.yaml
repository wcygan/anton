apiVersion: v1
kind: Pod
metadata:
  name: spark-iceberg-client
  namespace: data-platform
  labels:
    app: spark-iceberg-client
    data-platform.homelab/component: spark-client
spec:
  containers:
  - name: spark
    # TODO: Replace with your custom image build
    # For now, keeping with runtime download approach but documented for improvement
    # Custom image would be: homelab/spark-iceberg:3.5.5
    image: apache/spark:3.5.5-scala2.12-java11-python3-ubuntu
    command: ["sleep", "infinity"]
    env:
    - name: NESSIE_URI
      value: "http://nessie:19120/api/v2"
    - name: AWS_ENDPOINT_URL
      value: "http://rook-ceph-rgw-storage.storage.svc:80"
    - name: AWS_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: rook-ceph-object-user-storage-iceberg
          key: AccessKey
    - name: AWS_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: rook-ceph-object-user-storage-iceberg
          key: SecretKey
    - name: AWS_REGION
      value: "us-east-1"
    - name: SPARK_HOME
      value: "/opt/spark"
    - name: JAVA_HOME
      value: "/opt/java/openjdk"
    volumeMounts:
    - name: spark-conf
      mountPath: /opt/spark/conf/spark-defaults.conf
      subPath: spark-defaults.conf
    - name: iceberg-jars
      mountPath: /opt/spark/jars-iceberg
    resources:
      requests:
        memory: "2Gi"
        cpu: "500m"
      limits:
        memory: "4Gi"
        cpu: "2000m"
    workingDir: /opt/spark
    securityContext:
      runAsNonRoot: true
      runAsUser: 185  # spark user
      runAsGroup: 185
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: false  # Spark needs to write temp files
  volumes:
  - name: spark-conf
    configMap:
      name: spark-iceberg-config
  - name: iceberg-jars
    emptyDir: {}
  # NOTE: Keeping initContainer for now as interim solution
  # TODO: Replace with pre-built image using the provided Dockerfile
  initContainers:
  - name: download-iceberg-jars
    image: curlimages/curl:8.11.0
    command:
    - /bin/sh
    - -c
    - |
      set -e
      echo "Downloading Iceberg and AWS dependencies..."
      
      # Iceberg Spark runtime - updated version
      curl -L -o /jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar \
        https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.2/iceberg-spark-runtime-3.5_2.12-1.5.2.jar
      
      # Nessie Spark SQL extension
      curl -L -o /jars/nessie-spark-extensions-3.5_2.12-0.77.1.jar \
        https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.5_2.12/0.77.1/nessie-spark-extensions-3.5_2.12-0.77.1.jar
      
      # AWS S3 support
      curl -L -o /jars/hadoop-aws-3.3.4.jar \
        https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar
      
      curl -L -o /jars/aws-java-sdk-bundle-1.12.367.jar \
        https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.367/aws-java-sdk-bundle-1.12.367.jar
      
      echo "All dependencies downloaded successfully"
      ls -la /jars/
    volumeMounts:
    - name: iceberg-jars
      mountPath: /jars
    securityContext:
      runAsNonRoot: true
      runAsUser: 100  # curl user
      runAsGroup: 101
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
  restartPolicy: Never
  securityContext:
    runAsNonRoot: true
    runAsUser: 185
    runAsGroup: 185
    fsGroup: 185
    seccompProfile:
      type: RuntimeDefault
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-iceberg-config
  namespace: data-platform
data:
  spark-defaults.conf: |
    # Iceberg and Nessie Configuration
    spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions
    spark.sql.catalog.nessie=org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.nessie.catalog-impl=org.apache.iceberg.nessie.NessieCatalog
    spark.sql.catalog.nessie.uri=http://nessie:19120/api/v2
    spark.sql.catalog.nessie.ref=main
    spark.sql.catalog.nessie.warehouse=s3a://iceberg-test
    spark.sql.catalog.nessie.io-impl=org.apache.iceberg.aws.s3.S3FileIO
    
    # S3 Configuration for Ceph
    spark.hadoop.fs.s3a.endpoint=http://rook-ceph-rgw-storage.storage.svc:80
    spark.hadoop.fs.s3a.path.style.access=true
    spark.hadoop.fs.s3a.connection.ssl.enabled=false
    spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
    
    # Performance and Compatibility
    spark.sql.adaptive.enabled=true
    spark.sql.adaptive.coalescePartitions.enabled=true
    spark.serializer=org.apache.spark.serializer.KryoSerializer
    spark.sql.hive.convertMetastoreParquet=false
    
    # JAR Configuration
    spark.jars=/opt/spark/jars-iceberg/iceberg-spark-runtime-3.5_2.12-1.5.2.jar,/opt/spark/jars-iceberg/nessie-spark-extensions-3.5_2.12-0.77.1.jar,/opt/spark/jars-iceberg/hadoop-aws-3.3.4.jar,/opt/spark/jars-iceberg/aws-java-sdk-bundle-1.12.367.jar
    
    # Driver Configuration
    spark.driver.memory=2g
    spark.driver.maxResultSize=1g
# External secrets removed - using existing rook-ceph-object-user-storage-iceberg secret
# This secret is automatically created by the Ceph ObjectStore User in storage namespace